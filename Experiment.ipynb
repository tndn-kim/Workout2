{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.8894437209302326\n",
      "F1-score: 0.8894615794461835\n",
      "오탐: 0.1105562790697674\n",
      "미탐: 0.09294869901923486\n"
     ]
    }
   ],
   "source": [
    "# 전체레이블   \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "# 데이터 로드\n",
    "data = pd.read_csv(\"C:/Users/milab_8/Downloads/Data.csv\")\n",
    "label = pd.read_csv(\"C:/Users/milab_8/Downloads/Label.csv\")\n",
    "df = pd.concat([data, label], axis=1)\n",
    "\n",
    "# 특성과 라벨 분리\n",
    "x = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "\n",
    "# 데이터 정규화\n",
    "scaler = MinMaxScaler()\n",
    "x = scaler.fit_transform(x)\n",
    "\n",
    "# 학습 및 테스트 데이터 분리\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "\n",
    "def evaluate_model(model, x_test, y_test):\n",
    "    y_pred = model.predict(x_test)\n",
    "    print(\"정확도:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"F1-score:\", f1_score(y_test, y_pred, average='weighted'))\n",
    "    print(\"오탐:\", 1 - recall_score(y_test, y_pred, average='weighted', zero_division=0))\n",
    "    print(\"미탐:\", 1 - precision_score(y_test, y_pred, average='weighted', zero_division=0))\n",
    "\n",
    "# 로지스틱 회귀 모델\n",
    "print(\"Logistic Regression\")\n",
    "logistic_model = LogisticRegression(multi_class='multinomial', max_iter=2000)\n",
    "logistic_model.fit(x_train, y_train)\n",
    "evaluate_model(logistic_model, x_test, y_test)\n",
    "\n",
    "# 랜덤 포레스트 모델\n",
    "print(\"\\nRandom Forest\")\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(x_train, y_train)\n",
    "evaluate_model(rf_model, x_test, y_test)\n",
    "\n",
    "# LightGBM 모델\n",
    "print(\"\\nLightGBM\")\n",
    "lgb_model = lgb.LGBMClassifier(boosting_type='gbdt', objective='multiclass', num_leaves=31, learning_rate=0.1, n_estimators=100, random_state=42)\n",
    "lgb_model.fit(x_train, y_train)\n",
    "evaluate_model(lgb_model, x_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.9346902325581395\n",
      "F1-score: 0.9351172654194579\n",
      "오탐: 0.06530976744186046\n",
      "미탐: 0.06432223311657581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\milab_8\\.conda\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    }
   ],
   "source": [
    "#이진클래스\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "data = pd.read_csv(\"C:/Users/milab_8/Downloads/Data.csv\")\n",
    "label = pd.read_csv(\"C:/Users/milab_8/Downloads/Label.csv\")\n",
    "df=pd.concat([data,label],axis=1)\n",
    "x=df.iloc[:,:-1].values\n",
    "y=df.iloc[:,-1].values\n",
    "y=np.where(y != 0, 1 , y)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "x = scaler.fit_transform(x)\n",
    "\n",
    "# 학습 및 테스트 데이터 분리\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "def evaluate_model(model, x_test, y_test):\n",
    "    y_pred = model.predict(x_test)\n",
    "    print(\"정확도:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"F1-score:\", f1_score(y_test, y_pred, average='weighted'))\n",
    "    print(\"오탐:\", 1 - recall_score(y_test, y_pred, average='weighted', zero_division=0))\n",
    "    print(\"미탐:\", 1 - precision_score(y_test, y_pred, average='weighted', zero_division=0))\n",
    "\n",
    "# 로지스틱 회귀 모델\n",
    "print(\"Logistic Regression\")\n",
    "logistic_model = LogisticRegression(multi_class='multinomial', max_iter=2000)\n",
    "logistic_model.fit(x_train, y_train)\n",
    "evaluate_model(logistic_model, x_test, y_test)\n",
    "\n",
    "# 랜덤 포레스트 모델\n",
    "print(\"\\nRandom Forest\")\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(x_train, y_train)\n",
    "evaluate_model(rf_model, x_test, y_test)\n",
    "\n",
    "# LightGBM 모델\n",
    "print(\"\\nLightGBM\")\n",
    "lgb_model = lgb.LGBMClassifier(boosting_type='gbdt', objective='multiclass', num_leaves=31, learning_rate=0.1, n_estimators=100, random_state=42)\n",
    "lgb_model.fit(x_train, y_train)\n",
    "evaluate_model(lgb_model, x_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7268\\3497428620.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0msmote\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSMOTE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mx_re\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_re\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msmote\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\milab_8\\.conda\\envs\\myenv\\lib\\site-packages\\imblearn\\base.py\u001b[0m in \u001b[0;36mfit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    206\u001b[0m         \"\"\"\n\u001b[0;32m    207\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_more_tags\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\milab_8\\.conda\\envs\\myenv\\lib\\site-packages\\imblearn\\base.py\u001b[0m in \u001b[0;36mfit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    110\u001b[0m         )\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_resample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         y_ = (\n",
      "\u001b[1;32mc:\\Users\\milab_8\\.conda\\envs\\myenv\\lib\\site-packages\\imblearn\\over_sampling\\_smote\\base.py\u001b[0m in \u001b[0;36m_fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn_k_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 389\u001b[1;33m             \u001b[0mnns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn_k_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_distance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    390\u001b[0m             X_new, y_new = self._make_samples(\n\u001b[0;32m    391\u001b[0m                 \u001b[0mX_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_sample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\milab_8\\.conda\\envs\\myenv\\lib\\site-packages\\sklearn\\neighbors\\_base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[1;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    757\u001b[0m                     \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meffective_metric_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m                     \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 759\u001b[1;33m                     \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    760\u001b[0m                 )\n\u001b[0;32m    761\u001b[0m             )\n",
      "\u001b[1;32mc:\\Users\\milab_8\\.conda\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36mpairwise_distances_chunked\u001b[1;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[0m\n\u001b[0;32m   1724\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreduce_func\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1725\u001b[0m             \u001b[0mchunk_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mD_chunk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1726\u001b[1;33m             \u001b[0mD_chunk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreduce_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mD_chunk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1727\u001b[0m             \u001b[0m_check_chunk_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mD_chunk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1728\u001b[0m         \u001b[1;32myield\u001b[0m \u001b[0mD_chunk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\milab_8\\.conda\\envs\\myenv\\lib\\site-packages\\sklearn\\neighbors\\_base.py\u001b[0m in \u001b[0;36m_kneighbors_reduce_func\u001b[1;34m(self, dist, start, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    632\u001b[0m         \"\"\"\n\u001b[0;32m    633\u001b[0m         \u001b[0msample_range\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 634\u001b[1;33m         \u001b[0mneigh_ind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margpartition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_neighbors\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    635\u001b[0m         \u001b[0mneigh_ind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mneigh_ind\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m         \u001b[1;31m# argpartition doesn't guarantee sorted order, so we sort again\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36margpartition\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\milab_8\\.conda\\envs\\myenv\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36margpartition\u001b[1;34m(a, kth, axis, kind, order)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    838\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 839\u001b[1;33m \u001b[1;33m@\u001b[0m\u001b[0marray_function_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_sort_dispatcher\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    840\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    841\u001b[0m     \"\"\"\n",
      "\u001b[1;32mc:\\Users\\milab_8\\.conda\\envs\\myenv\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#이진클래스 oversampling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm as lgb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "data = pd.read_csv(\"C:/Users/milab_8/Downloads/Data.csv\")\n",
    "label = pd.read_csv(\"C:/Users/milab_8/Downloads/Label.csv\")\n",
    "df=pd.concat([data,label],axis=1)\n",
    "x=df.iloc[:,:-1].values\n",
    "y=df.iloc[:,-1].values\n",
    "y=np.where(y != 0, 1 , y)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "x = scaler.fit_transform(x)\n",
    "\n",
    "smote=SMOTE()\n",
    "\n",
    "x_re,y_re=smote.fit_resample(x,y)\n",
    "\n",
    "\n",
    "x_df=pd.DataFrame(x)\n",
    "x_re_df=pd.DataFrame(x_re,columns=x_df.columns)\n",
    "y_df=pd.Series(y)\n",
    "y_re_df=pd.Series(y_re)\n",
    "\n",
    "x_augmented = x_re_df\n",
    "y_augmented = y_re_df\n",
    "\n",
    "# 학습 및 테스트 데이터 분리\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_augmented, y_augmented, test_size=0.3, random_state=42)\n",
    "\n",
    "def evaluate_model(model, x_test, y_test):\n",
    "    y_pred = model.predict(x_test)\n",
    "    print(\"정확도:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"F1-score:\", f1_score(y_test, y_pred, average='weighted'))\n",
    "    print(\"오탐:\", 1 - recall_score(y_test, y_pred, average='weighted', zero_division=0))\n",
    "    print(\"미탐:\", 1 - precision_score(y_test, y_pred, average='weighted', zero_division=0))\n",
    "\n",
    "# 로지스틱 회귀 모델\n",
    "print(\"Logistic Regression\")\n",
    "logistic_model = LogisticRegression(multi_class='multinomial', max_iter=2000)\n",
    "logistic_model.fit(x_train, y_train)\n",
    "evaluate_model(logistic_model, x_test, y_test)\n",
    "\n",
    "# 랜덤 포레스트 모델\n",
    "print(\"\\nRandom Forest\")\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(x_train, y_train)\n",
    "evaluate_model(rf_model, x_test, y_test)\n",
    "\n",
    "# LightGBM 모델\n",
    "print(\"\\nLightGBM\")\n",
    "lgb_model = lgb.LGBMClassifier(boosting_type='gbdt', objective='multiclass', num_leaves=31, learning_rate=0.1, n_estimators=100, random_state=42)\n",
    "lgb_model.fit(x_train, y_train)\n",
    "evaluate_model(lgb_model, x_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 71666 samples, validate on 17917 samples\n",
      "Epoch 1/50\n",
      "71666/71666 [==============================] - 2s 24us/sample - loss: 0.2869 - val_loss: 0.1069\n",
      "Epoch 2/50\n",
      "19200/71666 [=======>......................] - ETA: 0s - loss: 0.0985"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\milab_8\\.conda\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2464: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71666/71666 [==============================] - 0s 6us/sample - loss: 0.0894 - val_loss: 0.0827\n",
      "Epoch 3/50\n",
      "71666/71666 [==============================] - 0s 6us/sample - loss: 0.0794 - val_loss: 0.0781\n",
      "Epoch 4/50\n",
      "71666/71666 [==============================] - 0s 6us/sample - loss: 0.0765 - val_loss: 0.0762\n",
      "Epoch 5/50\n",
      "71666/71666 [==============================] - 0s 6us/sample - loss: 0.0752 - val_loss: 0.0752\n",
      "Epoch 6/50\n",
      "71666/71666 [==============================] - 0s 6us/sample - loss: 0.0745 - val_loss: 0.0746\n",
      "Epoch 7/50\n",
      "71666/71666 [==============================] - 0s 6us/sample - loss: 0.0740 - val_loss: 0.0742\n",
      "Epoch 8/50\n",
      "71666/71666 [==============================] - 0s 6us/sample - loss: 0.0736 - val_loss: 0.0739\n",
      "Epoch 9/50\n",
      "71666/71666 [==============================] - 0s 6us/sample - loss: 0.0734 - val_loss: 0.0736\n",
      "Epoch 10/50\n",
      "71666/71666 [==============================] - 0s 6us/sample - loss: 0.0732 - val_loss: 0.0734\n",
      "Epoch 11/50\n",
      "71666/71666 [==============================] - 0s 6us/sample - loss: 0.0730 - val_loss: 0.0733\n",
      "Epoch 12/50\n",
      "71666/71666 [==============================] - 0s 6us/sample - loss: 0.0729 - val_loss: 0.0732\n",
      "Epoch 13/50\n",
      "71666/71666 [==============================] - 0s 6us/sample - loss: 0.0727 - val_loss: 0.0731\n",
      "Epoch 14/50\n",
      "71666/71666 [==============================] - 0s 6us/sample - loss: 0.0726 - val_loss: 0.0730\n",
      "Epoch 15/50\n",
      "71666/71666 [==============================] - 0s 6us/sample - loss: 0.0726 - val_loss: 0.0729\n",
      "Epoch 16/50\n",
      "71666/71666 [==============================] - 0s 6us/sample - loss: 0.0725 - val_loss: 0.0728\n",
      "Epoch 17/50\n",
      "71666/71666 [==============================] - 0s 6us/sample - loss: 0.0724 - val_loss: 0.0727\n",
      "Epoch 18/50\n",
      "71666/71666 [==============================] - 0s 6us/sample - loss: 0.0723 - val_loss: 0.0727\n",
      "Epoch 19/50\n",
      "71666/71666 [==============================] - 0s 7us/sample - loss: 0.0723 - val_loss: 0.0726\n",
      "Epoch 20/50\n",
      "71666/71666 [==============================] - 0s 7us/sample - loss: 0.0722 - val_loss: 0.0726\n",
      "Epoch 21/50\n",
      "71666/71666 [==============================] - 0s 7us/sample - loss: 0.0722 - val_loss: 0.0725\n",
      "Epoch 22/50\n",
      "71666/71666 [==============================] - 0s 6us/sample - loss: 0.0721 - val_loss: 0.0725\n",
      "Epoch 23/50\n",
      "71666/71666 [==============================] - 0s 6us/sample - loss: 0.0721 - val_loss: 0.0724\n",
      "Epoch 24/50\n",
      "71666/71666 [==============================] - 0s 6us/sample - loss: 0.0721 - val_loss: 0.0724\n",
      "Epoch 25/50\n",
      "71666/71666 [==============================] - 0s 6us/sample - loss: 0.0720 - val_loss: 0.0724\n",
      "Epoch 26/50\n",
      "71666/71666 [==============================] - 0s 6us/sample - loss: 0.0720 - val_loss: 0.0723\n",
      "Epoch 27/50\n",
      "71666/71666 [==============================] - 0s 6us/sample - loss: 0.0720 - val_loss: 0.0723\n",
      "Epoch 28/50\n",
      "71666/71666 [==============================] - 0s 7us/sample - loss: 0.0719 - val_loss: 0.0723\n",
      "Epoch 29/50\n",
      "71666/71666 [==============================] - 0s 6us/sample - loss: 0.0719 - val_loss: 0.0723\n",
      "Epoch 30/50\n",
      "71666/71666 [==============================] - 0s 6us/sample - loss: 0.0719 - val_loss: 0.0722\n",
      "Epoch 31/50\n",
      "71666/71666 [==============================] - 0s 6us/sample - loss: 0.0719 - val_loss: 0.0722\n",
      "Epoch 32/50\n",
      "71666/71666 [==============================] - 0s 6us/sample - loss: 0.0719 - val_loss: 0.0722\n",
      "Epoch 33/50\n",
      "71666/71666 [==============================] - 0s 6us/sample - loss: 0.0718 - val_loss: 0.0722\n",
      "Epoch 34/50\n",
      "71666/71666 [==============================] - 0s 6us/sample - loss: 0.0718 - val_loss: 0.0722\n",
      "Epoch 35/50\n",
      "71666/71666 [==============================] - 0s 7us/sample - loss: 0.0718 - val_loss: 0.0721\n",
      "Epoch 36/50\n",
      "71666/71666 [==============================] - 0s 6us/sample - loss: 0.0718 - val_loss: 0.0721\n",
      "Epoch 37/50\n",
      "71666/71666 [==============================] - 0s 6us/sample - loss: 0.0718 - val_loss: 0.0721\n",
      "Epoch 38/50\n",
      "71666/71666 [==============================] - 0s 6us/sample - loss: 0.0718 - val_loss: 0.0721\n",
      "Epoch 39/50\n",
      "71666/71666 [==============================] - 0s 6us/sample - loss: 0.0718 - val_loss: 0.0721\n",
      "Epoch 40/50\n",
      "71666/71666 [==============================] - 0s 6us/sample - loss: 0.0717 - val_loss: 0.0721\n",
      "Epoch 41/50\n",
      "71666/71666 [==============================] - 0s 6us/sample - loss: 0.0717 - val_loss: 0.0721\n",
      "Epoch 42/50\n",
      "71666/71666 [==============================] - 0s 6us/sample - loss: 0.0717 - val_loss: 0.0721\n",
      "Epoch 43/50\n",
      "71666/71666 [==============================] - 0s 6us/sample - loss: 0.0717 - val_loss: 0.0721\n",
      "Epoch 44/50\n",
      "71666/71666 [==============================] - 0s 6us/sample - loss: 0.0717 - val_loss: 0.0721\n",
      "Epoch 45/50\n",
      "71666/71666 [==============================] - 0s 6us/sample - loss: 0.0717 - val_loss: 0.0720\n",
      "Epoch 46/50\n",
      "71666/71666 [==============================] - 0s 6us/sample - loss: 0.0717 - val_loss: 0.0720\n",
      "Epoch 47/50\n",
      "71666/71666 [==============================] - 0s 6us/sample - loss: 0.0717 - val_loss: 0.0720\n",
      "Epoch 48/50\n",
      "71666/71666 [==============================] - 0s 6us/sample - loss: 0.0717 - val_loss: 0.0720\n",
      "Epoch 49/50\n",
      "71666/71666 [==============================] - 0s 6us/sample - loss: 0.0717 - val_loss: 0.0720\n",
      "Epoch 50/50\n",
      "71666/71666 [==============================] - 0s 7us/sample - loss: 0.0717 - val_loss: 0.0720\n",
      "정확도: 0.9841302325581396\n",
      "F1-score: 0.9841263342490154\n",
      "오탐: 0.015869767441860416\n",
      "미탐: 0.015455189040967099\n"
     ]
    }
   ],
   "source": [
    "#이진클래스 오토인코더\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score, precision_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "# 데이터 불러오기\n",
    "data = pd.read_csv(\"C:/Users/milab_8/Downloads/Data.csv\")\n",
    "label = pd.read_csv(\"C:/Users/milab_8/Downloads/Label.csv\")\n",
    "\n",
    "# 이진 분류를 위한 레이블 변환 (0과 1로 설정)\n",
    "label = np.where(label != 0, 1, label)  # 0이 아닌 값은 모두 1로 변환\n",
    "label = pd.DataFrame(label, columns=['label'])\n",
    "\n",
    "# 데이터프레임 병합\n",
    "df = pd.concat([data, label], axis=1)\n",
    "\n",
    "# label이 1인 데이터 추출 (증강할 데이터)\n",
    "over_data_1 = data.loc[label['label'] == 1]\n",
    "\n",
    "# label이 0인 데이터 추출 (증강 데이터와 결합할 나머지 데이터)\n",
    "other_data = data.loc[label['label'] == 0]\n",
    "other_label = label.loc[label['label'] == 0]\n",
    "\n",
    "# 데이터 스케일링\n",
    "scaler = MinMaxScaler()\n",
    "scaled_over_data = scaler.fit_transform(over_data_1)\n",
    "scaled_other_data = scaler.transform(other_data)  # 다른 데이터에 fit_transform을 하지 않고 transform만 진행\n",
    "\n",
    "# Autoencoder 모델 설정\n",
    "input_dim = scaled_over_data.shape[1]\n",
    "encoding_dim = input_dim // 2\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
    "\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# Autoencoder 학습\n",
    "autoencoder.fit(scaled_over_data, scaled_over_data, epochs=50, batch_size=256, shuffle=True, validation_split=0.2)\n",
    "\n",
    "# Autoencoder로 데이터 인코딩\n",
    "encoded_data = autoencoder.predict(scaled_over_data)\n",
    "\n",
    "# 증강 데이터 생성 함수\n",
    "def augment_data(encoded_data, target_size):\n",
    "    augmented_data = encoded_data\n",
    "    while len(augmented_data) < target_size:\n",
    "        augmented_data = np.concatenate((augmented_data, autoencoder.predict(encoded_data)), axis=0)\n",
    "    return augmented_data[:target_size]\n",
    "\n",
    "# label이 0인 데이터 수에 맞추기 위해 데이터 증강\n",
    "target_size = len(other_data)\n",
    "augmented_data_1 = augment_data(encoded_data, target_size)\n",
    "\n",
    "# 원본 데이터와 증강된 데이터 결합\n",
    "x_augmented = np.concatenate((scaled_other_data, augmented_data_1), axis=0)\n",
    "y_augmented = np.concatenate((other_label.values.flatten(), np.ones(target_size)), axis=0)  # 증강된 데이터의 라벨은 모두 1\n",
    "\n",
    "# 학습 및 테스트 데이터 분리\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_augmented, y_augmented, test_size=0.3, random_state=42)\n",
    "\n",
    "def evaluate_model(model, x_test, y_test):\n",
    "    y_pred = model.predict(x_test)\n",
    "    print(\"정확도:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"F1-score:\", f1_score(y_test, y_pred, average='weighted'))\n",
    "    print(\"오탐:\", 1 - recall_score(y_test, y_pred, average='weighted', zero_division=0))\n",
    "    print(\"미탐:\", 1 - precision_score(y_test, y_pred, average='weighted', zero_division=0))\n",
    "\n",
    "# 로지스틱 회귀 모델\n",
    "print(\"Logistic Regression\")\n",
    "logistic_model = LogisticRegression(multi_class='multinomial', max_iter=2000)\n",
    "logistic_model.fit(x_train, y_train)\n",
    "evaluate_model(logistic_model, x_test, y_test)\n",
    "\n",
    "# 랜덤 포레스트 모델\n",
    "print(\"\\nRandom Forest\")\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(x_train, y_train)\n",
    "evaluate_model(rf_model, x_test, y_test)\n",
    "\n",
    "# LightGBM 모델\n",
    "print(\"\\nLightGBM\")\n",
    "lgb_model = lgb.LGBMClassifier(boosting_type='gbdt', objective='multiclass', num_leaves=31, learning_rate=0.1, n_estimators=100, random_state=42)\n",
    "lgb_model.fit(x_train, y_train)\n",
    "evaluate_model(lgb_model, x_test, y_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.8979497674418605\n",
      "F1-score: 0.9038362068737441\n",
      "오탐: 0.1020502325581395\n",
      "미탐: 0.08541969574449393\n"
     ]
    }
   ],
   "source": [
    "#멀티클래스\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"C:/Users/milab_8/Downloads/Data.csv\")\n",
    "label = pd.read_csv(\"C:/Users/milab_8/Downloads/Label.csv\")\n",
    "label=np.where((label==1) | (label==7),1,label)\n",
    "label=np.where((label==3)| (label==5),3,label)\n",
    "label=np.where((label==0)|(label==1)|(label==3),label,2)\n",
    "label=pd.DataFrame(label,columns=['label'])\n",
    "df=pd.concat([data,label],axis=1)\n",
    "x=df.iloc[:,:-1]\n",
    "y=df.iloc[:,-1]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "x = scaler.fit_transform(x)\n",
    "# 학습 및 테스트 데이터 분리\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "\n",
    "def evaluate_model(model, x_test, y_test):\n",
    "    y_pred = model.predict(x_test)\n",
    "    print(\"정확도:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"F1-score:\", f1_score(y_test, y_pred, average='weighted'))\n",
    "    print(\"오탐:\", 1 - recall_score(y_test, y_pred, average='weighted', zero_division=0))\n",
    "    print(\"미탐:\", 1 - precision_score(y_test, y_pred, average='weighted', zero_division=0))\n",
    "\n",
    "# 로지스틱 회귀 모델\n",
    "print(\"Logistic Regression\")\n",
    "logistic_model = LogisticRegression(multi_class='multinomial', max_iter=2000)\n",
    "logistic_model.fit(x_train, y_train)\n",
    "evaluate_model(logistic_model, x_test, y_test)\n",
    "\n",
    "# 랜덤 포레스트 모델\n",
    "print(\"\\nRandom Forest\")\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(x_train, y_train)\n",
    "evaluate_model(rf_model, x_test, y_test)\n",
    "\n",
    "# LightGBM 모델\n",
    "print(\"\\nLightGBM\")\n",
    "lgb_model = lgb.LGBMClassifier(boosting_type='gbdt', objective='multiclass', num_leaves=31, learning_rate=0.1, n_estimators=100, random_state=42)\n",
    "lgb_model.fit(x_train, y_train)\n",
    "evaluate_model(lgb_model, x_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.717206319084463\n",
      "F1-score: 0.7197775803529253\n",
      "오탐: 0.282793680915537\n",
      "미탐: 0.26747094508184577\n"
     ]
    }
   ],
   "source": [
    "#멀티클래스 전체 oversampling\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,f1_score\n",
    "from sklearn.metrics import recall_score, precision_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "data = pd.read_csv(\"C:/Users/milab_8/Downloads/Data.csv\")\n",
    "label = pd.read_csv(\"C:/Users/milab_8/Downloads/Label.csv\")\n",
    "label=np.where((label==1) | (label==7),1,label)\n",
    "label=np.where((label==3)| (label==5),3,label)\n",
    "label=np.where((label==0)|(label==1)|(label==3),label,2)\n",
    "label=pd.DataFrame(label,columns=['label'])\n",
    "df=pd.concat([data,label],axis=1)\n",
    "x=df.iloc[:,:-1]\n",
    "y=df.iloc[:,-1]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "x = scaler.fit_transform(x)\n",
    "\n",
    "smote=SMOTE()\n",
    "\n",
    "x_re,y_re=smote.fit_resample(x,y)\n",
    "\n",
    "x_df=pd.DataFrame(x)\n",
    "x_re_df=pd.DataFrame(x_re,columns=x_df.columns)\n",
    "y_df=pd.Series(y)\n",
    "y_re_df=pd.Series(y_re)\n",
    "\n",
    "x_augmented = x_re_df\n",
    "y_augmented = y_re_df\n",
    "\n",
    "# 학습 및 테스트 데이터 분리\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_augmented, y_augmented, test_size=0.3, random_state=42)\n",
    "\n",
    "def evaluate_model(model, x_test, y_test):\n",
    "    y_pred = model.predict(x_test)\n",
    "    print(\"정확도:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"F1-score:\", f1_score(y_test, y_pred, average='weighted'))\n",
    "    print(\"오탐:\", 1 - recall_score(y_test, y_pred, average='weighted', zero_division=0))\n",
    "    print(\"미탐:\", 1 - precision_score(y_test, y_pred, average='weighted', zero_division=0))\n",
    "\n",
    "# 로지스틱 회귀 모델\n",
    "print(\"Logistic Regression\")\n",
    "logistic_model = LogisticRegression(multi_class='multinomial', max_iter=2000)\n",
    "logistic_model.fit(x_train, y_train)\n",
    "evaluate_model(logistic_model, x_test, y_test)\n",
    "\n",
    "# 랜덤 포레스트 모델\n",
    "print(\"\\nRandom Forest\")\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(x_train, y_train)\n",
    "evaluate_model(rf_model, x_test, y_test)\n",
    "\n",
    "# LightGBM 모델\n",
    "print(\"\\nLightGBM\")\n",
    "lgb_model = lgb.LGBMClassifier(boosting_type='gbdt', objective='multiclass', num_leaves=31, learning_rate=0.1, n_estimators=100, random_state=42)\n",
    "lgb_model.fit(x_train, y_train)\n",
    "evaluate_model(lgb_model, x_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13696 samples, validate on 3424 samples\n",
      "Epoch 1/50\n",
      "13696/13696 [==============================] - 0s 17us/sample - loss: 0.6094 - val_loss: 0.4588\n",
      "Epoch 2/50\n",
      "13696/13696 [==============================] - 0s 10us/sample - loss: 0.3081 - val_loss: 0.2164\n",
      "Epoch 3/50\n",
      "  256/13696 [..............................] - ETA: 0s - loss: 0.2067"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\milab_8\\.conda\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2464: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13696/13696 [==============================] - 0s 9us/sample - loss: 0.1866 - val_loss: 0.1732\n",
      "Epoch 4/50\n",
      "13696/13696 [==============================] - 0s 8us/sample - loss: 0.1591 - val_loss: 0.1542\n",
      "Epoch 5/50\n",
      "13696/13696 [==============================] - 0s 6us/sample - loss: 0.1452 - val_loss: 0.1437\n",
      "Epoch 6/50\n",
      "13696/13696 [==============================] - 0s 7us/sample - loss: 0.1372 - val_loss: 0.1373\n",
      "Epoch 7/50\n",
      "13696/13696 [==============================] - 0s 7us/sample - loss: 0.1319 - val_loss: 0.1323\n",
      "Epoch 8/50\n",
      "13696/13696 [==============================] - 0s 6us/sample - loss: 0.1277 - val_loss: 0.1284\n",
      "Epoch 9/50\n",
      "13696/13696 [==============================] - 0s 7us/sample - loss: 0.1247 - val_loss: 0.1257\n",
      "Epoch 10/50\n",
      "13696/13696 [==============================] - 0s 7us/sample - loss: 0.1226 - val_loss: 0.1239\n",
      "Epoch 11/50\n",
      "13696/13696 [==============================] - 0s 6us/sample - loss: 0.1210 - val_loss: 0.1225\n",
      "Epoch 12/50\n",
      "13696/13696 [==============================] - 0s 7us/sample - loss: 0.1199 - val_loss: 0.1214\n",
      "Epoch 13/50\n",
      "13696/13696 [==============================] - 0s 7us/sample - loss: 0.1189 - val_loss: 0.1205\n",
      "Epoch 14/50\n",
      "13696/13696 [==============================] - 0s 7us/sample - loss: 0.1182 - val_loss: 0.1199\n",
      "Epoch 15/50\n",
      "13696/13696 [==============================] - 0s 6us/sample - loss: 0.1176 - val_loss: 0.1193\n",
      "Epoch 16/50\n",
      "13696/13696 [==============================] - 0s 6us/sample - loss: 0.1171 - val_loss: 0.1188\n",
      "Epoch 17/50\n",
      "13696/13696 [==============================] - 0s 6us/sample - loss: 0.1167 - val_loss: 0.1185\n",
      "Epoch 18/50\n",
      "13696/13696 [==============================] - 0s 7us/sample - loss: 0.1164 - val_loss: 0.1181\n",
      "Epoch 19/50\n",
      "13696/13696 [==============================] - 0s 6us/sample - loss: 0.1161 - val_loss: 0.1179\n",
      "Epoch 20/50\n",
      "13696/13696 [==============================] - 0s 6us/sample - loss: 0.1158 - val_loss: 0.1177\n",
      "Epoch 21/50\n",
      "13696/13696 [==============================] - 0s 7us/sample - loss: 0.1156 - val_loss: 0.1174\n",
      "Epoch 22/50\n",
      "13696/13696 [==============================] - 0s 6us/sample - loss: 0.1154 - val_loss: 0.1173\n",
      "Epoch 23/50\n",
      "13696/13696 [==============================] - 0s 6us/sample - loss: 0.1152 - val_loss: 0.1171\n",
      "Epoch 24/50\n",
      "13696/13696 [==============================] - 0s 6us/sample - loss: 0.1151 - val_loss: 0.1169\n",
      "Epoch 25/50\n",
      "13696/13696 [==============================] - 0s 6us/sample - loss: 0.1149 - val_loss: 0.1168\n",
      "Epoch 26/50\n",
      "13696/13696 [==============================] - 0s 6us/sample - loss: 0.1148 - val_loss: 0.1167\n",
      "Epoch 27/50\n",
      "13696/13696 [==============================] - 0s 6us/sample - loss: 0.1147 - val_loss: 0.1166\n",
      "Epoch 28/50\n",
      "13696/13696 [==============================] - 0s 6us/sample - loss: 0.1146 - val_loss: 0.1165\n",
      "Epoch 29/50\n",
      "13696/13696 [==============================] - 0s 6us/sample - loss: 0.1145 - val_loss: 0.1164\n",
      "Epoch 30/50\n",
      "13696/13696 [==============================] - 0s 6us/sample - loss: 0.1144 - val_loss: 0.1163\n",
      "Epoch 31/50\n",
      "13696/13696 [==============================] - 0s 6us/sample - loss: 0.1143 - val_loss: 0.1162\n",
      "Epoch 32/50\n",
      "13696/13696 [==============================] - 0s 6us/sample - loss: 0.1142 - val_loss: 0.1161\n",
      "Epoch 33/50\n",
      "13696/13696 [==============================] - 0s 6us/sample - loss: 0.1141 - val_loss: 0.1161\n",
      "Epoch 34/50\n",
      "13696/13696 [==============================] - 0s 6us/sample - loss: 0.1141 - val_loss: 0.1160\n",
      "Epoch 35/50\n",
      "13696/13696 [==============================] - 0s 7us/sample - loss: 0.1140 - val_loss: 0.1159\n",
      "Epoch 36/50\n",
      "13696/13696 [==============================] - 0s 6us/sample - loss: 0.1140 - val_loss: 0.1159\n",
      "Epoch 37/50\n",
      "13696/13696 [==============================] - 0s 6us/sample - loss: 0.1139 - val_loss: 0.1158\n",
      "Epoch 38/50\n",
      "13696/13696 [==============================] - 0s 6us/sample - loss: 0.1138 - val_loss: 0.1158\n",
      "Epoch 39/50\n",
      "13696/13696 [==============================] - 0s 7us/sample - loss: 0.1138 - val_loss: 0.1157\n",
      "Epoch 40/50\n",
      "13696/13696 [==============================] - 0s 6us/sample - loss: 0.1137 - val_loss: 0.1157\n",
      "Epoch 41/50\n",
      "13696/13696 [==============================] - 0s 6us/sample - loss: 0.1137 - val_loss: 0.1156\n",
      "Epoch 42/50\n",
      "13696/13696 [==============================] - 0s 6us/sample - loss: 0.1137 - val_loss: 0.1156\n",
      "Epoch 43/50\n",
      "13696/13696 [==============================] - 0s 6us/sample - loss: 0.1136 - val_loss: 0.1155\n",
      "Epoch 44/50\n",
      "13696/13696 [==============================] - 0s 6us/sample - loss: 0.1136 - val_loss: 0.1155\n",
      "Epoch 45/50\n",
      "13696/13696 [==============================] - 0s 6us/sample - loss: 0.1135 - val_loss: 0.1155\n",
      "Epoch 46/50\n",
      "13696/13696 [==============================] - 0s 6us/sample - loss: 0.1135 - val_loss: 0.1154\n",
      "Epoch 47/50\n",
      "13696/13696 [==============================] - 0s 6us/sample - loss: 0.1135 - val_loss: 0.1154\n",
      "Epoch 48/50\n",
      "13696/13696 [==============================] - 0s 7us/sample - loss: 0.1134 - val_loss: 0.1154\n",
      "Epoch 49/50\n",
      "13696/13696 [==============================] - 0s 6us/sample - loss: 0.1134 - val_loss: 0.1153\n",
      "Epoch 50/50\n",
      "13696/13696 [==============================] - 0s 6us/sample - loss: 0.1134 - val_loss: 0.1153\n",
      "Train on 30706 samples, validate on 7677 samples\n",
      "Epoch 1/50\n",
      "30706/30706 [==============================] - 0s 6us/sample - loss: -3179.0056 - val_loss: -6724.4954\n",
      "Epoch 2/50\n",
      "30706/30706 [==============================] - 0s 7us/sample - loss: -10081.4562 - val_loss: -16176.3504\n",
      "Epoch 3/50\n",
      "30706/30706 [==============================] - 0s 7us/sample - loss: -20466.2857 - val_loss: -27962.5252\n",
      "Epoch 4/50\n",
      "30706/30706 [==============================] - 0s 7us/sample - loss: -33436.9912 - val_loss: -43711.0331\n",
      "Epoch 5/50\n",
      "30706/30706 [==============================] - 0s 7us/sample - loss: -49129.6827 - val_loss: -61537.0189\n",
      "Epoch 6/50\n",
      "30706/30706 [==============================] - 0s 6us/sample - loss: -66937.6921 - val_loss: -81848.3034\n",
      "Epoch 7/50\n",
      "30706/30706 [==============================] - 0s 6us/sample - loss: -86308.6264 - val_loss: -101965.4603\n",
      "Epoch 8/50\n",
      "30706/30706 [==============================] - 0s 6us/sample - loss: -106874.5872 - val_loss: -123067.3176\n",
      "Epoch 9/50\n",
      "30706/30706 [==============================] - 0s 6us/sample - loss: -128581.0345 - val_loss: -147325.0747\n",
      "Epoch 10/50\n",
      "30706/30706 [==============================] - 0s 6us/sample - loss: -151455.0804 - val_loss: -172992.0245\n",
      "Epoch 11/50\n",
      "30706/30706 [==============================] - 0s 6us/sample - loss: -176016.1501 - val_loss: -198230.0604\n",
      "Epoch 12/50\n",
      "30706/30706 [==============================] - 0s 6us/sample - loss: -201497.9707 - val_loss: -226996.4116\n",
      "Epoch 13/50\n",
      "30706/30706 [==============================] - 0s 6us/sample - loss: -227990.7123 - val_loss: -255051.3298\n",
      "Epoch 14/50\n",
      "30706/30706 [==============================] - 0s 6us/sample - loss: -254660.8809 - val_loss: -283987.5558\n",
      "Epoch 15/50\n",
      "30706/30706 [==============================] - 0s 6us/sample - loss: -282712.5720 - val_loss: -314389.8939\n",
      "Epoch 16/50\n",
      "30706/30706 [==============================] - 0s 7us/sample - loss: -312183.9914 - val_loss: -344589.2437\n",
      "Epoch 17/50\n",
      "30706/30706 [==============================] - 0s 7us/sample - loss: -342068.1283 - val_loss: -377571.1622\n",
      "Epoch 18/50\n",
      "30706/30706 [==============================] - 0s 9us/sample - loss: -373267.0668 - val_loss: -410678.9199\n",
      "Epoch 19/50\n",
      "30706/30706 [==============================] - 0s 6us/sample - loss: -405390.4877 - val_loss: -442198.2322\n",
      "Epoch 20/50\n",
      "30706/30706 [==============================] - 0s 6us/sample - loss: -438044.5166 - val_loss: -477523.7338\n",
      "Epoch 21/50\n",
      "30706/30706 [==============================] - 0s 6us/sample - loss: -471493.9441 - val_loss: -513696.3400\n",
      "Epoch 22/50\n",
      "30706/30706 [==============================] - 0s 6us/sample - loss: -505593.5075 - val_loss: -549017.0330\n",
      "Epoch 23/50\n",
      "30706/30706 [==============================] - 0s 6us/sample - loss: -540447.3444 - val_loss: -587365.4973\n",
      "Epoch 24/50\n",
      "30706/30706 [==============================] - 0s 6us/sample - loss: -576042.0694 - val_loss: -625310.6070\n",
      "Epoch 25/50\n",
      "30706/30706 [==============================] - 0s 6us/sample - loss: -612152.7537 - val_loss: -663607.0320\n",
      "Epoch 26/50\n",
      "30706/30706 [==============================] - 0s 5us/sample - loss: -650026.0276 - val_loss: -703854.8844\n",
      "Epoch 27/50\n",
      "30706/30706 [==============================] - 0s 6us/sample - loss: -688330.4574 - val_loss: -745407.5891\n",
      "Epoch 28/50\n",
      "30706/30706 [==============================] - 0s 6us/sample - loss: -728091.2319 - val_loss: -786953.9771\n",
      "Epoch 29/50\n",
      "30706/30706 [==============================] - 0s 6us/sample - loss: -768882.3378 - val_loss: -829688.2517\n",
      "Epoch 30/50\n",
      "30706/30706 [==============================] - 0s 6us/sample - loss: -810706.3977 - val_loss: -872075.1468\n",
      "Epoch 31/50\n",
      "30706/30706 [==============================] - 0s 6us/sample - loss: -853075.0722 - val_loss: -917823.0468\n",
      "Epoch 32/50\n",
      "30706/30706 [==============================] - 0s 6us/sample - loss: -896750.1717 - val_loss: -963480.2378\n",
      "Epoch 33/50\n",
      "30706/30706 [==============================] - 0s 7us/sample - loss: -940931.9617 - val_loss: -1010866.4331\n",
      "Epoch 34/50\n",
      "30706/30706 [==============================] - 0s 7us/sample - loss: -985985.9322 - val_loss: -1059187.8634\n",
      "Epoch 35/50\n",
      "30706/30706 [==============================] - 0s 7us/sample - loss: -1031183.5094 - val_loss: -1106957.9380\n",
      "Epoch 36/50\n",
      "30706/30706 [==============================] - 0s 6us/sample - loss: -1077816.7395 - val_loss: -1156577.2988\n",
      "Epoch 37/50\n",
      "30706/30706 [==============================] - 0s 6us/sample - loss: -1124454.2105 - val_loss: -1207315.2879\n",
      "Epoch 38/50\n",
      "30706/30706 [==============================] - 0s 6us/sample - loss: -1172240.8500 - val_loss: -1257776.2293\n",
      "Epoch 39/50\n",
      "30706/30706 [==============================] - 0s 7us/sample - loss: -1220850.9570 - val_loss: -1305868.2253\n",
      "Epoch 40/50\n",
      "30706/30706 [==============================] - 0s 6us/sample - loss: -1270455.8519 - val_loss: -1358793.7768\n",
      "Epoch 41/50\n",
      "30706/30706 [==============================] - 0s 6us/sample - loss: -1320107.0702 - val_loss: -1412598.8040\n",
      "Epoch 42/50\n",
      "30706/30706 [==============================] - 0s 6us/sample - loss: -1371536.4580 - val_loss: -1465139.9889\n",
      "Epoch 43/50\n",
      "30706/30706 [==============================] - 0s 6us/sample - loss: -1423691.8111 - val_loss: -1521694.2750\n",
      "Epoch 44/50\n",
      "30706/30706 [==============================] - 0s 6us/sample - loss: -1476584.1158 - val_loss: -1578229.4334\n",
      "Epoch 45/50\n",
      "30706/30706 [==============================] - 0s 7us/sample - loss: -1530491.0401 - val_loss: -1634255.4114\n",
      "Epoch 46/50\n",
      "30706/30706 [==============================] - 0s 6us/sample - loss: -1585393.3485 - val_loss: -1691202.5459\n",
      "Epoch 47/50\n",
      "30706/30706 [==============================] - 0s 7us/sample - loss: -1640288.3450 - val_loss: -1751636.0953\n",
      "Epoch 48/50\n",
      "30706/30706 [==============================] - 0s 7us/sample - loss: -1696048.5662 - val_loss: -1808929.7286\n",
      "Epoch 49/50\n",
      "30706/30706 [==============================] - 0s 7us/sample - loss: -1752746.7210 - val_loss: -1866251.6463\n",
      "Epoch 50/50\n",
      "30706/30706 [==============================] - 0s 7us/sample - loss: -1810354.9793 - val_loss: -1929713.7295\n",
      "Train on 27264 samples, validate on 6816 samples\n",
      "Epoch 1/50\n",
      "27264/27264 [==============================] - 0s 7us/sample - loss: -2705897.4307 - val_loss: -1969661.5183\n",
      "Epoch 2/50\n",
      "27264/27264 [==============================] - 0s 6us/sample - loss: -2829376.4335 - val_loss: -2043081.7032\n",
      "Epoch 3/50\n",
      "27264/27264 [==============================] - 0s 6us/sample - loss: -2929436.6707 - val_loss: -2104498.5200\n",
      "Epoch 4/50\n",
      "27264/27264 [==============================] - 0s 7us/sample - loss: -3019607.8828 - val_loss: -2172482.0011\n",
      "Epoch 5/50\n",
      "27264/27264 [==============================] - 0s 6us/sample - loss: -3104355.9363 - val_loss: -2229149.3156\n",
      "Epoch 6/50\n",
      "27264/27264 [==============================] - 0s 6us/sample - loss: -3184571.1580 - val_loss: -2280638.6653\n",
      "Epoch 7/50\n",
      "27264/27264 [==============================] - 0s 6us/sample - loss: -3260394.9397 - val_loss: -2340226.2730\n",
      "Epoch 8/50\n",
      "27264/27264 [==============================] - 0s 6us/sample - loss: -3339170.2519 - val_loss: -2392565.2538\n",
      "Epoch 9/50\n",
      "27264/27264 [==============================] - 0s 7us/sample - loss: -3413702.5711 - val_loss: -2444102.4265\n",
      "Epoch 10/50\n",
      "27264/27264 [==============================] - 0s 7us/sample - loss: -3488638.0255 - val_loss: -2496871.5324\n",
      "Epoch 11/50\n",
      "27264/27264 [==============================] - 0s 6us/sample - loss: -3562857.7463 - val_loss: -2547044.3007\n",
      "Epoch 12/50\n",
      "27264/27264 [==============================] - 0s 7us/sample - loss: -3634573.4262 - val_loss: -2595429.6852\n",
      "Epoch 13/50\n",
      "27264/27264 [==============================] - 0s 6us/sample - loss: -3706787.8864 - val_loss: -2653445.1176\n",
      "Epoch 14/50\n",
      "27264/27264 [==============================] - 0s 8us/sample - loss: -3782312.7809 - val_loss: -2702970.3954\n",
      "Epoch 15/50\n",
      "27264/27264 [==============================] - 0s 7us/sample - loss: -3854819.6286 - val_loss: -2753818.2939\n",
      "Epoch 16/50\n",
      "27264/27264 [==============================] - 0s 6us/sample - loss: -3926683.6415 - val_loss: -2805391.2481\n",
      "Epoch 17/50\n",
      "27264/27264 [==============================] - 0s 6us/sample - loss: -4001036.2574 - val_loss: -2852094.0684\n",
      "Epoch 18/50\n",
      "27264/27264 [==============================] - 0s 7us/sample - loss: -4072796.8666 - val_loss: -2905358.3840\n",
      "Epoch 19/50\n",
      "27264/27264 [==============================] - 0s 6us/sample - loss: -4147029.8205 - val_loss: -2954851.2542\n",
      "Epoch 20/50\n",
      "27264/27264 [==============================] - 0s 7us/sample - loss: -4218714.2215 - val_loss: -3008077.1856\n",
      "Epoch 21/50\n",
      "27264/27264 [==============================] - 0s 6us/sample - loss: -4291968.0816 - val_loss: -3059619.6828\n",
      "Epoch 22/50\n",
      "27264/27264 [==============================] - 0s 7us/sample - loss: -4366410.0426 - val_loss: -3110747.2752\n",
      "Epoch 23/50\n",
      "27264/27264 [==============================] - 0s 7us/sample - loss: -4439659.5873 - val_loss: -3165351.7110\n",
      "Epoch 24/50\n",
      "27264/27264 [==============================] - 0s 6us/sample - loss: -4514455.3836 - val_loss: -3214274.6708\n",
      "Epoch 25/50\n",
      "27264/27264 [==============================] - 0s 6us/sample - loss: -4590381.5970 - val_loss: -3273639.6147\n",
      "Epoch 26/50\n",
      "27264/27264 [==============================] - 0s 6us/sample - loss: -4666518.6368 - val_loss: -3325213.0907\n",
      "Epoch 27/50\n",
      "27264/27264 [==============================] - 0s 6us/sample - loss: -4741422.1948 - val_loss: -3378467.2835\n",
      "Epoch 28/50\n",
      "27264/27264 [==============================] - 0s 7us/sample - loss: -4816721.2905 - val_loss: -3431830.8780\n",
      "Epoch 29/50\n",
      "27264/27264 [==============================] - 0s 6us/sample - loss: -4893154.9591 - val_loss: -3486579.6603\n",
      "Epoch 30/50\n",
      "27264/27264 [==============================] - 0s 6us/sample - loss: -4969882.5919 - val_loss: -3537319.1190\n",
      "Epoch 31/50\n",
      "27264/27264 [==============================] - 0s 6us/sample - loss: -5045194.0643 - val_loss: -3589877.8050\n",
      "Epoch 32/50\n",
      "27264/27264 [==============================] - 0s 7us/sample - loss: -5121579.3921 - val_loss: -3646300.2204\n",
      "Epoch 33/50\n",
      "27264/27264 [==============================] - 0s 7us/sample - loss: -5199235.6531 - val_loss: -3698928.7065\n",
      "Epoch 34/50\n",
      "27264/27264 [==============================] - 0s 6us/sample - loss: -5276168.2166 - val_loss: -3753780.2823\n",
      "Epoch 35/50\n",
      "27264/27264 [==============================] - 0s 6us/sample - loss: -5354230.1423 - val_loss: -3808498.6738\n",
      "Epoch 36/50\n",
      "27264/27264 [==============================] - 0s 6us/sample - loss: -5437105.0932 - val_loss: -3865580.8862\n",
      "Epoch 37/50\n",
      "27264/27264 [==============================] - 0s 6us/sample - loss: -5518053.2774 - val_loss: -3926288.3773\n",
      "Epoch 38/50\n",
      "27264/27264 [==============================] - 0s 6us/sample - loss: -5597986.7778 - val_loss: -3982550.4799\n",
      "Epoch 39/50\n",
      "27264/27264 [==============================] - 0s 6us/sample - loss: -5676702.7743 - val_loss: -4036659.8191\n",
      "Epoch 40/50\n",
      "27264/27264 [==============================] - 0s 7us/sample - loss: -5756432.8814 - val_loss: -4094687.1848\n",
      "Epoch 41/50\n",
      "27264/27264 [==============================] - 0s 9us/sample - loss: -5836824.1173 - val_loss: -4150809.1864\n",
      "Epoch 42/50\n",
      "27264/27264 [==============================] - 0s 6us/sample - loss: -5914000.6584 - val_loss: -4210969.4292\n",
      "Epoch 43/50\n",
      "27264/27264 [==============================] - 0s 6us/sample - loss: -5996755.3757 - val_loss: -4266351.8109\n",
      "Epoch 44/50\n",
      "27264/27264 [==============================] - 0s 6us/sample - loss: -6077208.4030 - val_loss: -4316624.3074\n",
      "Epoch 45/50\n",
      "27264/27264 [==============================] - 0s 6us/sample - loss: -6159657.0710 - val_loss: -4377146.8623\n",
      "Epoch 46/50\n",
      "27264/27264 [==============================] - 0s 6us/sample - loss: -6240390.1644 - val_loss: -4434878.3293\n",
      "Epoch 47/50\n",
      "27264/27264 [==============================] - 0s 6us/sample - loss: -6324358.8439 - val_loss: -4496093.8806\n",
      "Epoch 48/50\n",
      "27264/27264 [==============================] - 0s 6us/sample - loss: -6407147.8731 - val_loss: -4552891.6111\n",
      "Epoch 49/50\n",
      "27264/27264 [==============================] - 0s 6us/sample - loss: -6492104.0760 - val_loss: -4611575.5823\n",
      "Epoch 50/50\n",
      "27264/27264 [==============================] - 0s 6us/sample - loss: -6577369.0050 - val_loss: -4675233.7651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\milab_8\\.conda\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.9602487447645227\n",
      "F1-score: 0.960171690458404\n",
      "오탐: 0.03975125523547729\n",
      "미탐: 0.039448135530075534\n"
     ]
    }
   ],
   "source": [
    "#멀티클래스 오토인코더\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "# 데이터 불러오기\n",
    "data = pd.read_csv(\"C:/Users/milab_8/Downloads/Data.csv\")\n",
    "label = pd.read_csv(\"C:/Users/milab_8/Downloads/Label.csv\")\n",
    "\n",
    "# 라벨 변환\n",
    "label = np.where((label == 1) | (label == 7), 1, label)\n",
    "label = np.where((label == 3) | (label == 5), 3, label)\n",
    "label = np.where((label == 0) | (label == 1) | (label == 3), label, 2)\n",
    "label = pd.DataFrame(label, columns=['label'])\n",
    "\n",
    "# 데이터프레임 병합\n",
    "df = pd.concat([data, label], axis=1)\n",
    "\n",
    "# label이 1, 2, 3인 데이터 추출\n",
    "over_data_1 = data.loc[label['label'] == 1]\n",
    "over_data_2 = data.loc[label['label'] == 2]\n",
    "over_data_3 = data.loc[label['label'] == 3]\n",
    "\n",
    "# label이 0인 데이터 추출 (증강 데이터와 결합하기 위한 나머지 데이터)\n",
    "other_data = data.loc[label['label'] == 0]\n",
    "other_label = label.loc[label['label'] == 0]\n",
    "\n",
    "# 데이터 스케일링\n",
    "scaler = MinMaxScaler()\n",
    "scaled_over_data_1 = scaler.fit_transform(over_data_1)\n",
    "scaled_over_data_2 = scaler.transform(over_data_2)\n",
    "scaled_over_data_3 = scaler.transform(over_data_3)\n",
    "scaled_other_data = scaler.transform(other_data)\n",
    "\n",
    "# Autoencoder 모델 설정\n",
    "input_dim = scaled_over_data_1.shape[1]  # 입력 차원 설정\n",
    "encoding_dim = input_dim // 2\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
    "\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# Autoencoder 학습 (각 클래스에 대해 별도로 수행)\n",
    "autoencoder.fit(scaled_over_data_1, scaled_over_data_1, epochs=50, batch_size=256, shuffle=True, validation_split=0.2)\n",
    "encoded_data_1 = autoencoder.predict(scaled_over_data_1)\n",
    "\n",
    "autoencoder.fit(scaled_over_data_2, scaled_over_data_2, epochs=50, batch_size=256, shuffle=True, validation_split=0.2)\n",
    "encoded_data_2 = autoencoder.predict(scaled_over_data_2)\n",
    "\n",
    "autoencoder.fit(scaled_over_data_3, scaled_over_data_3, epochs=50, batch_size=256, shuffle=True, validation_split=0.2)\n",
    "encoded_data_3 = autoencoder.predict(scaled_over_data_3)\n",
    "\n",
    "# 증강 데이터 양을 조절하기 위해 각 클래스별로 증강\n",
    "target_size = len(other_data)  # label이 0인 데이터 수에 맞추기\n",
    "\n",
    "def augment_data(encoded_data, target_size):\n",
    "    augmented_data = encoded_data\n",
    "    while len(augmented_data) < target_size:\n",
    "        augmented_data = np.concatenate((augmented_data, autoencoder.predict(encoded_data)), axis=0)\n",
    "    return augmented_data[:target_size]\n",
    "\n",
    "augmented_data_1 = augment_data(encoded_data_1, target_size)\n",
    "augmented_data_2 = augment_data(encoded_data_2, target_size)\n",
    "augmented_data_3 = augment_data(encoded_data_3, target_size)\n",
    "\n",
    "# 원본 데이터와 인코딩된 데이터 결합\n",
    "x_augmented = np.concatenate((scaled_other_data, augmented_data_1, augmented_data_2, augmented_data_3), axis=0)\n",
    "y_augmented = np.concatenate((other_label.values.flatten(), np.ones(target_size), np.full(target_size, 2), np.full(target_size, 3)), axis=0)\n",
    "\n",
    "# 학습 및 테스트 데이터 분리\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_augmented, y_augmented, test_size=0.3, random_state=42)\n",
    "\n",
    "def evaluate_model(model, x_test, y_test):\n",
    "    y_pred = model.predict(x_test)\n",
    "    print(\"정확도:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"F1-score:\", f1_score(y_test, y_pred, average='weighted'))\n",
    "    print(\"오탐:\", 1 - recall_score(y_test, y_pred, average='weighted', zero_division=0))\n",
    "    print(\"미탐:\", 1 - precision_score(y_test, y_pred, average='weighted', zero_division=0))\n",
    "\n",
    "# 로지스틱 회귀 모델\n",
    "print(\"Logistic Regression\")\n",
    "logistic_model = LogisticRegression(multi_class='multinomial', max_iter=2000)\n",
    "logistic_model.fit(x_train, y_train)\n",
    "evaluate_model(logistic_model, x_test, y_test)\n",
    "\n",
    "# 랜덤 포레스트 모델\n",
    "print(\"\\nRandom Forest\")\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(x_train, y_train)\n",
    "evaluate_model(rf_model, x_test, y_test)\n",
    "\n",
    "# LightGBM 모델\n",
    "print(\"\\nLightGBM\")\n",
    "lgb_model = lgb.LGBMClassifier(boosting_type='gbdt', objective='multiclass', num_leaves=31, learning_rate=0.1, n_estimators=100, random_state=42)\n",
    "lgb_model.fit(x_train, y_train)\n",
    "evaluate_model(lgb_model, x_test, y_test)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
